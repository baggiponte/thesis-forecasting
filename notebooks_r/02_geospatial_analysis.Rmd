---
jupyter:
  jupytext:
    formats: notebooks///ipynb,notebooks_r///Rmd
    main_language: python
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.4
  kernelspec:
    display_name: 'Python 3.8.10 64-bit (''bikemi_win'': conda)'
    name: python3
---

# Import Libraries and Load Data

```{python}
import pandas as pd
import geopandas

import matplotlib.pyplot as plt

import custom_functions.joins as j
import custom_functions.clean_strings as cs
```

Let's import the data. `geopandas.GeoDataFrame` can have as many `geopandas.GeoSeries` objects within, but only one is active at a time.

```{python}
# load stalls lon-lat table
bikemi_stalls = geopandas.read_file("../data/bikemi_metadata/bikemi_stalls.geojson")

# load NIL lon-lat table
nil = geopandas.read_file("../data/milan/milan_nil.geojson")

# actually not needed, municipi categorical var is already in bikemi_stalls
municipi = geopandas.read_file("../data/milan/milan_municipi.geojson")

```

```{python}
# select only the column we need + rename
nil = nil[["ID_NIL", "NIL", "geometry"]] \
    .assign(NIL = lambda x: x["NIL"].str.title()) \
    .astype({"ID_NIL": "category", "NIL": "category"}) \
    .rename(columns={"NIL": "nil", "ID_NIL": "nil_number"}) \
    .set_index("nil_number") \
    .sort_values("nil", ascending = True)
```

```{python}
# select only the column we need + rename
bikemi_stalls = bikemi_stalls \
    [["nome", "zd_attuale", "geometry", "anno"]] \
    .rename(columns={"zd_attuale" : "municipio"}) \
    .sort_values(by = ["nome"], ascending = True) \
    .astype({"nome": "category", "municipio": "category"}) \
    .set_index("nome")
```

```{python}
bikemi_stalls
```

We have geometries. Let's make a first plot, to see how it looks like.

```{python}
# define ax object and dimensions
fig, ax = plt.subplots(figsize = (10,10))

# plot both objects on the same axes; order matters
nil.plot(ax = ax, cmap = "Blues")
bikemi_stalls.plot(ax = ax, color = "tab:red")

# remove the axis lines
plt.axis("off")
plt.show()
```

Finally, let's load the time series data:

```{python}
# load the time series for each station in the long format
station_outflow = pd.read_csv("../data/bikemi_csv/station_daily_outflow.csv", parse_dates=[0], index_col=[0])

station_outflow.info()
```

# First Join: Bikemi Stalls and Time Series Data

```{python}
# join the index of the table specified as the first argument on the column specified by the "on" argument
tentative_join = geopandas.GeoDataFrame(
    # the data to pass as first argument:
    station_outflow.join(bikemi_stalls, on = "stazione_partenza") \
    .sort_values(["giorno_partenza", "stazione_partenza"], ascending = True),
    # then set the coordinate reference system
    crs = "EPSG:4326"
)
```

```{python}
tentative_join.info()
```

Some information is actually lost! There are some observations from the dataset which cannot be matched to the stalls list.

```{python}
unique_stalls = pd.Series(bikemi_stalls.index.unique())
unique_stations = pd.Series(station_outflow["stazione_partenza"].sort_values(ascending=True).unique(), name = "stazione_partenza")

print(f"""
Number of stalls in the official data: {unique_stalls.size}.
Number of stalls in our time series: {unique_stations.size}.
""")
```

13 stalls are missing, but we are likely to find more mismatches. To deal with this, we have to turn to snake_case all station names, then perform the join again. We will have to use regular expressions. As Jamie Zawinski said in 1997:

> Some people, when confronted with a problem, think: "I know, I'll use regular expressions". Now they have two problems.

Here we apply some custom functions, which can be found in the omonimous folder under this directory.

```{python}
clean_stalls = cs.clean_series(unique_stalls)
clean_stations = cs.clean_series(unique_stations)

j.mismatches(clean_stalls, clean_stations, left_on = "nome", right_on = "stazione_partenza")
mismatched_stations, mismatched_stalls = j.mismatches(clean_stalls, clean_stations, left_on = "nome", right_on = "stazione_partenza", text = False)

```

```{python}
list(mismatched_stalls), list(mismatched_stations)
```

Some stations do not appear because they were introduced in 2020 and 2021, i.e. `Brunelleschi - Giambellino`. Others, such as `Gioia - Vespucci`, do not have a value for `anno`.

```{python}
bikemi_stalls.loc[(bikemi_stalls["anno"] >= 2020) | (bikemi_stalls["anno"].isna())]
# does not work in this case, but:
# bikemi_stalls.loc[bikemi_stalls["anno"].isin([2020,2021])]
```

This is another reason for aggregating the data to the NIL level: stations may change across this time span, whereas NIL stay the same for 10 years at least.

Now we can retry the join, after applying the custom function for cleaning strings to each column we plan to join.

```{python}
clean_outflow = cs.clean_df(station_outflow, col = "stazione_partenza", inplace = True)
clean_bikemi_stalls = cs.clean_df(bikemi_stalls.drop("anno", axis = 1).reset_index(), col = "nome", inplace = True).set_index("nome")
```

```{python}
geo_outflow = geopandas.GeoDataFrame(
    clean_outflow.join(clean_bikemi_stalls, on = "stazione_partenza").iloc[::-1],
    crs = "EPSG:4326"
)
```

```{python}
geo_outflow.info()
```

Notice that we lose just a small part of the observations. Now it's finally time to perform the spatial joins to obtain the NIL from this data:


# Spatial Joins


Let's proceed with a spatial operation, to assign each stall to its NIL, i.e. neighbourhood. The spatial operations can be:

* `intersects`: if the object on the left is inside or on the boundary of the object on the right.
* `contains`: if the object on the left contains the one on the right (i.e. no part of right is outside of left).
* `within`: the opposite of `contains`.
* `touches`: if boundaries touch in at least one point, but none in the interior.
* `crosses`: more than one but not all points in common.
* `overlaps`: same as above, but the objects have the same dimension and yet do not coincide (see it as equivalence vs congruence).

```{python}
geo_outflow_nil = geopandas.sjoin(geo_outflow, nil, how="inner", op = "intersects")
```

```{python}
geo_outflow_nil = geo_outflow_nil.rename(columns = {"geometry": "nil_geometry", "index_right" : "nil_number", "nil": "nil_name"}).set_geometry("nil_geometry")

geo_outflow_nil.info()
```

```{python}
geo_outflow_nil.head()
```

```{python}
geo_outflow_nil.nil_name.unique()
```

Basically, there are bikemi stations in only half of the NILs in Milan.

```{python}
geo_outflow_nil.reset_index().groupby(["nil_name", "giorno_partenza"]).sum().reset_index().set_index("giorno_partenza")
```

Finally, we might want to have preserved the `municipio` information:

```{python}
nil_municipi = geopandas.sjoin(nil, municipi, how = "inner", op = "intersects")

nil_municipi.head()
```

```{python}

```

```{python}
nil_municipi = nil_municipi[["nil", "MUNICIPIO", "geometry"]].rename(columns= {"nil": "nil_name", "MUNICIPIO": "municipio", "geometry": "geometry_municipio"})\
    .join(nil).reset_index().rename(columns={"geometry": "geometry_nil"}).drop("nil", axis = 1)
```

The shape of the dataframe tells us that some `NIL` actually end un in multiple `Municipi`: for example, Brera spans across three!

# Save the Final Dataset

```{python}
geo_outflow_nil.to_csv("../data/bikemi_csv/nil_daily_outflow.csv")
```
