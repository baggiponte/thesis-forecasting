{"data":{"layout-restorer:data":{"main":{"dock":{"type":"tab-area","currentIndex":1,"widgets":["notebook:tesi/notebooks/04-data_transform_and_stationarity.ipynb","notebook:tesi/notebooks/05-ARIMA.ipynb","editor:tesi/notebooks/notes.md"]},"current":"notebook:tesi/notebooks/05-ARIMA.ipynb"},"down":{"size":0,"widgets":[]},"left":{"collapsed":false,"current":"filebrowser","widgets":["filebrowser","running-sessions","git-sessions","@jupyterlab/toc:plugin","jupyterlab-citation-manager:reference-browser","extensionmanager.main-view"]},"right":{"collapsed":true,"widgets":["jp-property-inspector","debugger-sidebar"]},"relativeSizes":[0.16970138383102695,0.830298616168973,0]},"file-browser-filebrowser:cwd":{"path":"tesi"},"@jupyterlab/settingeditor-extension:plugin":{"sizes":[0.2020573108008817,0.7979426891991183],"container":{"plugin":"@jupyterlab/codemirror-extension:commands","sizes":[0.4737327188940092,0.5262672811059907]}},"notebook:tesi/notebooks/04-data_transform_and_stationarity.ipynb":{"data":{"path":"tesi/notebooks/04-data_transform_and_stationarity.ipynb","factory":"Notebook"}},"notebook:tesi/notebooks/05-ARIMA.ipynb":{"data":{"path":"tesi/notebooks/05-ARIMA.ipynb","factory":"Notebook"}},"editor:tesi/notebooks/notes.md":{"data":{"path":"tesi/notebooks/notes.md","factory":"Editor"}},"jupyterlab-citation-manager:zotero":{"persistentCacheVersion":"0..","apiVersion":"3","lastModifiedLibraryVersion":"688","citableItems":{"7765261/AWXEQJT6":{"id":"7765261/AWXEQJT6","type":"article","title":"Full Text PDF","URL":"https://media.proquest.com/media/pq/classic/doc/2858934771/fmt/pi/rep/NONE?cit%3Aauth=Cleveland%2C+Robert+B%3BCleveland%2C+William+S%3BTerpenning%2C+Irma&cit%3Atitle=STL%3A+A+Seasonal-Trend+Decomposition+Procedure+Based+on+Loess&cit%3Apub=Journal+of+Official+Statistics&cit%3Avol=6&cit%3Aiss=1&cit%3Apg=3&cit%3Adate=Mar+1990&ic=true&cit%3Aprod=ProQuest&_a=ChgyMDIxMDgyOTE5NTczMTM2Njo2OTMzNTYSBjEwMDk1OBoKT05FX1NFQVJDSCINMTU5LjE0OS4xMDMuOSoGMTA1NDQ0MgoxMjY2ODA1OTg5Og1Eb2N1bWVudEltYWdlQgEwUgZPbmxpbmVaAkZUYgNQRlRqCjE5OTAvMDMvMDFyCjE5OTAvMDMvMzF6AIIBKVAtMTAwNzIwMC0xMjQ1OS1DVVNUT01FUi0xMDAwMDI2Mi01MDQ0ODc2kgEGT25saW5lygF5TW96aWxsYS81LjAgKE1hY2ludG9zaDsgSW50ZWwgTWFjIE9TIFggMTBfMTVfNykgQXBwbGVXZWJLaXQvNTM3LjM2IChLSFRNTCwgbGlrZSBHZWNrbykgQ2hyb21lLzkyLjAuNDUxNS4xNTkgU2FmYXJpLzUzNy4zNtIBElNjaG9sYXJseSBKb3VybmFsc5oCB1ByZVBhaWSqAiVPUzpFTVMtRG93bmxvYWRQZGYtZ2V0TWVkaWFVcmxGb3JJdGVtygIPRmVhdHVyZXxBcnRpY2xl0gIBWeICAU7yAgD6AgFZggMDV2ViigMcQ0lEOjIwMjEwODI5MTk1NzMxMzk2OjgwOTgxMA%3D%3D&_s=nrMQ7KLX89rVl%2Buph3sW1hx%2BcIY%3D","accessed":{"date-parts":[[2021,8,29]]}},"7765261/JEKFIWNG":{"id":"7765261/JEKFIWNG","type":"article-journal","title":"STL: A Seasonal-Trend Decomposition Procedure Based on Loess","container-title":"Journal of Official Statistics","page":"3","volume":"6","issue":"1","abstract":"STL is a filtering procedure for decomposing a time series into trend, seasonal, and remainder components. STL has a simple design that consists of a sequence of applications of the loess smoother; the simplicity allows analysis of the properties of the procedure and allows fast computation, even for very long time series and large amounts of trend and seasonal smoothing. Other features of STL are specification of amounts of seasonal and trend smoothing that range, in a nearly continuous way, from a very small amount of smoothing to a very large amount; robust estimates of the trend and seasonal components that are not distorted by aberrant behavior in the data; specification of the period of the seasonal component to any integer multiple of the time sampling interval greater than one; and the ability to decompose time series with missing values.","URL":"http://www.proquest.com/docview/1266805989/abstract/A3E02EAC7236478APQ/1","note":"Num Pages: 3\nPlace: Stockholm, Sweden\nPublisher: Statistics Sweden (SCB)","shortTitle":"STL","language":"English","author":[{"family":"Cleveland","given":"Robert B."},{"family":"Cleveland","given":"William S."},{"family":"Terpenning","given":"Irma"}],"issued":{"date-parts":[["1990",3]]},"accessed":{"date-parts":[[2021,8,29]]}},"7765261/VHEQBWDS":{"id":"7765261/VHEQBWDS","type":"article","title":"Springer Full Text PDF","URL":"http://link.springer.com/content/pdf/10.1007%2Fs11207-019-1434-6.pdf","accessed":{"date-parts":[[2021,4,28]]}},"7765261/T4LR5VI6":{"id":"7765261/T4LR5VI6","type":"article-journal","title":"Forecasting Sunspot Time Series Using Deep Learning Methods","container-title":"Solar Physics","page":"50","volume":"294","issue":"5","abstract":"To predict Solar Cycle 25, we used the values of sunspot number (SSN), which have been measured regularly from 1749 to the present. In this study, we converted the SSN dataset, which consists of SSNs between 1749 – 2018, into a time series, and made the ten-year forecast with the help of deep-learning (DL) algorithms. Our results show that algorithms such as long-short-term memory (LSTM) and neural network autoregression (NNAR), which are DL algorithms, perform better than many algorithms such as ARIMA, Naive, Seasonal Naive, Mean and Drift, which are expressed as classical algorithms in a large time-series estimation process. Using the R programming language, it was also predicted that the maximum amplitude of Solar Cycle (SC) 25 will be reached between 2022 and 2023.","URL":"https://doi.org/10.1007/s11207-019-1434-6","DOI":"10.1007/s11207-019-1434-6","journalAbbreviation":"Sol Phys","language":"en","author":[{"family":"Pala","given":"Zeydin"},{"family":"Atici","given":"Ramazan"}],"issued":{"date-parts":[[2019,5,2]]},"accessed":{"date-parts":[[2021,4,28]]}},"7765261/PTD5XI2Q":{"id":"7765261/PTD5XI2Q","type":"article","title":"Koenecke - Applying Deep Neural Networks to Financial Time Se.pdf","URL":"https://stanford.edu/~koenecke/files/Deep_Learning_for_Time_Series_Tutorial.pdf","accessed":{"date-parts":[[2021,4,15]]}},"7765261/TPYELQPV":{"id":"7765261/TPYELQPV","type":"article-journal","title":"Applying Deep Neural Networks to Financial Time Series Forecasting","page":"22","abstract":"For any ﬁnancial organization, forecasting economic and ﬁnancial variables is a critical operation. As the granularity at which forecasts are needed increases, traditional statistical time series models may not scale well; on the other hand, it is easy to incorrectly overﬁt machine learning models. In this chapter, we will describe the basics of traditional time series analyses, discuss how neural networks work, show how to implement time series forecasting using neural networks, and ﬁnally present an example with real data from Microsoft. In particular, Microsoft successfully approached revenue forecasting using deep neural networks conjointly with curriculum learning, which achieved a higher level of precision than is possible with traditional techniques.","language":"en","author":[{"family":"Koenecke","given":"Allison"}]},"7765261/HLDKRAT9":{"id":"7765261/HLDKRAT9","type":"article","title":"Full Text PDF","URL":"https://arxiv.org/pdf/1704.04110.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/EADBEBJI":{"id":"7765261/EADBEBJI","type":"article","title":"Semantic Scholar Link","URL":"https://www.semanticscholar.org/paper/DeepAR%3A-Probabilistic-Forecasting-with-Recurrent-Flunkert-Salinas/4eebe0d12aefeedf3ca85256bc8aa3b4292d47d9","accessed":{"date-parts":[[2021,4,14]]}},"7765261/AIP7WCKT":{"id":"7765261/AIP7WCKT","type":"article-journal","title":"DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks","container-title":"ArXiv","abstract":"Probabilistic forecasting, i.e. estimating the probability distribution of a time series' future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an auto regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15% compared to state-of-the-art methods.","DOI":"10.1016/j.ijforecast.2019.07.001","shortTitle":"DeepAR","author":[{"family":"Flunkert","given":"Valentin"},{"family":"Salinas","given":"David"},{"family":"Gasthaus","given":"Jan"}],"issued":{"date-parts":[[2017]]}},"7765261/6GLI688R":{"id":"7765261/6GLI688R","type":"article","title":"ScienceDirect Snapshot","URL":"https://www.sciencedirect.com/science/article/pii/S2352711019300202?via%3Dihub","accessed":{"date-parts":[[2021,4,14]]}},"7765261/TY38F7IQ":{"id":"7765261/TY38F7IQ","type":"article","title":"ScienceDirect Full Text PDF","URL":"https://pdf.sciencedirectassets.com/312019/1-s2.0-S2352711020X00021/1-s2.0-S2352711019300202/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCl%2F4IuJCDX%2FZM63zaa0AaqXccmyY2L2AVYATAmA%2BKryAIhAJldkWIuwA7Gu0ugwhe1TERb63p1kUcvVoJ%2Ba3Crc2FJKr0DCPH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMDU5MDAzNTQ2ODY1Igw3EP1IdhsPScxJyxsqkQMKVesm5%2B%2FnpO4N505NZXkVYoHZOaoHVl%2Fo%2B%2FH%2F4VdIUso0VZWeWBDa2nRdYnVRLaiUme%2Fuy4Ew3edtrDu0CD81I0ha5vfUh20Hf7Cy3%2F7SOUq37rF%2Fihtki9gCPXtuFILS%2FCnNvUlH3UM%2FA6Ga4okSqgypkUivSC6r17IVa%2BSq01D%2BKfktr%2FTYcdXx6e0MX5NwkS5bV%2BnOY9jA0YA5Zwf2SYllZEixUM1FZNRgdv6sZddi8oCFsnRKHbemW8g9ULPzvvqdbqTK9mipVe4Oh40L7wJxm4HehH4ahWJDu0fqXiVuO3EWRY7Mixtw1PE35V1XwQhWs73u9ydcn%2Bx%2BsW%2FLxhLDUQIq44lTHVBxjFqvgd2Fby7P7%2BtOVw6E383v19S2dwBaR%2FAXOT9u7xzeUEzWZNwTZKXWchhsVi5KWa2IOG18XiUt%2BHOqRMyYFdfBbIP2xQU%2FMfda4HZeB3CVdFXHz0q%2BF%2FNXUqr7Foq9GEAiOcWplxmLKNgcZ%2BJrA0wogJ8VaNNC6he2Z1oVzWwYTzy84TCImNyDBjrqAYPONNCu3mCUjZJJcfbgCgpcj4uonVZTjr0XzZ24QvFMrzdxwteW9QKWz4qEcknpHpn9omhYi7ALXE6tftHi%2Fq1Xoa5GjxQLMqGg%2Fdos4DKgcHhyu1mM6Stsj5UK50p35G4o0H1VHE2gvdsPKiel2ZP%2Bf4GYI4%2BL18iSbgk3f%2BZvVzhadJlHAGk4Nc4hf6qG3PTs%2B6c2nH7xKHqLfBvd8cKEwyZFvNhNbqW6kSDosIjQW3imxBC4swUBo7%2FkCr7PHljsfSFuRSihGgZlp0aD9DGPyC3Jh1oSmwNhq3mQ9%2B7b3eXJQ5en049WUg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210414T160748Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYX7XFJE7V%2F20210414%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=6dab60a022f938a88912cd5ddd393be5a5a54c6cd8e0f960c385a97aab5c6cdc&hash=cc67af44b80ecb72faca8048c17311145ce259ac8e6de2cce34224780c130a76&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S2352711019300202&tid=spdf-5aa01228-2673-4a7f-b390-a3fe7ddbfb1f&sid=85d04e1c4e52c544a37875e0afc3a974e051gxrqb&type=client","accessed":{"date-parts":[[2021,4,14]]}},"7765261/K3BYSTLZ":{"id":"7765261/K3BYSTLZ","type":"article-journal","title":"Mcfly: Automated deep learning on time series","container-title":"SoftwareX","page":"100548","volume":"12","abstract":"Deep learning is receiving increasing attention in the scientific community, but for researchers with no or limited machine learning experience it can be difficult to get started. In particular, the so-called hyperparameter selection, which is critical to successfully train a model, requires a good understanding of deep learning and some experience training models. We present mcfly, a Python package for deep learning for time series classification, designed to ease training by providing automated hyperparameter selection. We evaluated mcfly by organizing two workshops.","URL":"https://www.sciencedirect.com/science/article/pii/S2352711019300202","DOI":"10.1016/j.softx.2020.100548","shortTitle":"Mcfly","journalAbbreviation":"SoftwareX","language":"en","author":[{"family":"van Kuppevelt","given":"D."},{"family":"Meijer","given":"C."},{"family":"Huber","given":"F."},{"family":"van der Ploeg","given":"A."},{"family":"Georgievska","given":"S."},{"family":"van Hees","given":"V. T."}],"issued":{"date-parts":[["2020",7,1]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/84LYNMDQ":{"id":"7765261/84LYNMDQ","type":"article","title":"Tavenard et al. - Tslearn, A Machine Learning Toolkit for Time Serie.pdf","URL":"https://jmlr.csail.mit.edu/papers/volume21/20-091/20-091.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/K8RFWNBR":{"id":"7765261/K8RFWNBR","type":"article-journal","title":"Tslearn, A Machine Learning Toolkit for Time Series Data","page":"6","language":"en","author":[{"family":"Tavenard","given":"Romain"},{"family":"Faouzi","given":"Johann"},{"family":"Vandewiele","given":"Gilles"},{"family":"Divo","given":"Felix"},{"family":"Androz","given":"Guillaume"},{"family":"Holtz","given":"Chester"},{"family":"Payne","given":"Marie"},{"family":"Yurchak","given":"Roman"},{"family":"Rußwurm","given":"Marc"},{"family":"Kolar","given":"Kushal"},{"family":"Woods","given":"Eli"}]},"7765261/PLP8GZNN":{"id":"7765261/PLP8GZNN","type":"article","title":"Full Text PDF","URL":"https://arxiv.org/pdf/1909.07872.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/63UQHDQ5":{"id":"7765261/63UQHDQ5","type":"article","title":"Semantic Scholar Link","URL":"https://www.semanticscholar.org/paper/sktime%3A-A-Unified-Interface-for-Machine-Learning-L%C3%B6ning-Bagnall/38eb43ea485e7d288c75ee514e0d51b8bffa3d18","accessed":{"date-parts":[[2021,4,14]]}},"7765261/JULTRBMT":{"id":"7765261/JULTRBMT","type":"article-journal","title":"sktime: A Unified Interface for Machine Learning with Time Series","container-title":"ArXiv","abstract":"We present sktime -- a new scikit-learn compatible Python library with a unified interface for machine learning with time series. Time series data gives rise to various distinct but closely related learning tasks, such as forecasting and time series classification, many of which can be solved by reducing them to related simpler tasks. We discuss the main rationale for creating a unified interface, including reduction, as well as the design of sktime's core API, supported by a clear overview of common time series tasks and reduction approaches.","shortTitle":"sktime","author":[{"family":"Löning","given":"M."},{"family":"Bagnall","given":"A."},{"family":"Ganesh","given":"Sajaysurya"},{"family":"Kazakov","given":"V."},{"family":"Lines","given":"J."},{"family":"Király","given":"F."}],"issued":{"date-parts":[[2019]]}},"7765261/PF6AAEMN":{"id":"7765261/PF6AAEMN","type":"article","title":"Full Text PDF","URL":"https://arxiv.org/pdf/1906.05264.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/7HIHURGU":{"id":"7765261/7HIHURGU","type":"article-journal","title":"GluonTS: Probabilistic Time Series Models in Python","container-title":"ArXiv","abstract":"We introduce Gluon Time Series (GluonTS, available at this https URL), a library for deep-learning-based time series modeling. GluonTS simplifies the development of and experimentation with time series models for common tasks such as forecasting or anomaly detection. It provides all necessary components and tools that scientists need for quickly building new models, for efficiently running and analyzing experiments and for evaluating model accuracy.","shortTitle":"GluonTS","author":[{"family":"Alexandrov","given":"Alexander"},{"family":"Benidis","given":"Konstantinos"},{"family":"Bohlke-Schneider","given":"Michael"},{"family":"Flunkert","given":"Valentin"},{"family":"Gasthaus","given":"Jan"},{"family":"Januschowski","given":"Tim"},{"family":"Maddix","given":"Danielle C."},{"family":"Rangapuram","given":"Syama Sundar"},{"family":"Salinas","given":"David"},{"family":"Schulz","given":"J."},{"family":"Stella","given":"Lorenzo"},{"family":"Türkmen","given":"Ali Caner"},{"family":"Wang","given":"Y."}],"issued":{"date-parts":[[2019]]}},"7765261/4VQGCM8R":{"id":"7765261/4VQGCM8R","type":"article","title":"Semantic Scholar Link","URL":"https://www.semanticscholar.org/paper/GluonTS%3A-Probabilistic-Time-Series-Models-in-Python-Alexandrov-Benidis/1f49343bc186c075434db8ac92cc1653c242661b","accessed":{"date-parts":[[2021,4,14]]}},"7765261/HPTMSPT7":{"id":"7765261/HPTMSPT7","type":"article","title":"Full Text PDF","URL":"https://arxiv.org/pdf/2004.10240.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/HXSDBZL5":{"id":"7765261/HXSDBZL5","type":"article","title":"Semantic Scholar Link","URL":"https://www.semanticscholar.org/paper/Neural-forecasting%3A-Introduction-and-literature-Benidis-Rangapuram/9f3eef3283597f9e5199897e8db6334b2d64a614","accessed":{"date-parts":[[2021,4,14]]}},"7765261/LC38YEZ6":{"id":"7765261/LC38YEZ6","type":"article-journal","title":"Neural forecasting: Introduction and literature overview","container-title":"ArXiv","abstract":"Neural network based forecasting methods have become ubiquitous in large-scale industrial forecasting applications over the last years. As the prevalence of neural network based solutions among the best entries in the recent M4 competition shows, the recent popularity of neural forecasting methods is not limited to industry and has also reached academia. This article aims at providing an introduction and an overview of some of the advances that have permitted the resurgence of neural networks in machine learning. Building on these foundations, the article then gives an overview of the recent literature on neural networks for forecasting and applications.","shortTitle":"Neural forecasting","author":[{"family":"Benidis","given":"Konstantinos"},{"family":"Rangapuram","given":"Syama Sundar"},{"family":"Flunkert","given":"Valentin"},{"family":"Wang","given":"Bernie"},{"family":"Maddix","given":"Danielle C."},{"family":"Türkmen","given":"Ali Caner"},{"family":"Gasthaus","given":"Jan"},{"family":"Bohlke-Schneider","given":"Michael"},{"family":"Salinas","given":"David"},{"family":"Stella","given":"Lorenzo"},{"family":"Callot","given":"Laurent"},{"family":"Januschowski","given":"Tim"}],"issued":{"date-parts":[[2020]]}},"7765261/QLMBDMFC":{"id":"7765261/QLMBDMFC","type":"article","title":"Springer Full Text PDF","URL":"https://link.springer.com/content/pdf/10.1007%2Fs10618-019-00619-1.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/PI3RQTMP":{"id":"7765261/PI3RQTMP","type":"article-journal","title":"Deep learning for time series classification: a review","container-title":"Data Mining and Knowledge Discovery","page":"917-963","volume":"33","issue":"4","abstract":"Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.","URL":"https://doi.org/10.1007/s10618-019-00619-1","DOI":"10.1007/s10618-019-00619-1","shortTitle":"Deep learning for time series classification","journalAbbreviation":"Data Min Knowl Disc","language":"en","author":[{"family":"Ismail Fawaz","given":"Hassan"},{"family":"Forestier","given":"Germain"},{"family":"Weber","given":"Jonathan"},{"family":"Idoumghar","given":"Lhassane"},{"family":"Muller","given":"Pierre-Alain"}],"issued":{"date-parts":[[2019,7,1]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/IB79PG9D":{"id":"7765261/IB79PG9D","type":"article","title":"Full Text PDF","URL":"https://arxiv.org/pdf/2101.02118.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/8SWMX8X7":{"id":"7765261/8SWMX8X7","type":"article","title":"Semantic Scholar Link","URL":"https://www.semanticscholar.org/paper/Do-We-Really-Need-Deep-Learning-Models-for-Time-Elsayed-Thyssens/4efd04d65924cf074d334489982a1218ee36adea","accessed":{"date-parts":[[2021,4,14]]}},"7765261/T2B4TSDG":{"id":"7765261/T2B4TSDG","type":"article-journal","title":"Do We Really Need Deep Learning Models for Time Series Forecasting?","container-title":"ArXiv","abstract":"Time series forecasting is a crucial task in machine learning, as it has a wide range of applications including but not limited to forecasting electricity consumption, traffic, and air quality. Traditional forecasting models relied on rolling averages, vector auto-regression and auto-regressive integrated moving averages. On the other hand, deep learning and matrix factorization models have been recently proposed to tackle the same problem with more competitive performance. However, one major drawback of such models is that they tend to be overly complex in comparison to traditional techniques.In this paper, we try to answer whether these highly complex deep learning models are without alternative. We aim to enrich the pool of simple but powerful baselines by revisiting the gradient boosting regression trees for time series forecasting. Specifically, we reconfigure the way time series data is handled by Gradient Tree Boosting models in a windowed fashion that is similar to the deep learning models. For each training window, the target values are concatenated with external features, and then flattened to form one input instance for a multi-output gradient boosting regression tree model. We conducted a comparative study on nine datasets for eight state-of-theart deep-learning models that were presented at top-level conferences in the last years. The results demonstrated that the proposed approach outperforms all of the state-of-the-","author":[{"family":"Elsayed","given":"Shereen"},{"family":"Thyssens","given":"Daniela"},{"family":"Rashed","given":"Ahmed"},{"family":"Schmidt-Thieme","given":"L."},{"family":"Jomaa","given":"H."}],"issued":{"date-parts":[[2021]]}},"7765261/EQA4MT8I":{"id":"7765261/EQA4MT8I","type":"article","title":"IEEE Xplore Abstract Record","URL":"https://ieeexplore.ieee.org/document/9079512","accessed":{"date-parts":[[2021,4,14]]}},"7765261/D8LNCKF8":{"id":"7765261/D8LNCKF8","type":"article","title":"IEEE Xplore Full Text PDF","URL":"https://ieeexplore.ieee.org/ielx7/6287639/8948470/09079512.pdf?tp=&arnumber=9079512&isnumber=8948470&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkwNzk1MTI=","accessed":{"date-parts":[[2021,4,14]]}},"7765261/967LGCRA":{"id":"7765261/967LGCRA","type":"article-journal","title":"Traffic Flow Forecast Through Time Series Analysis Based on Deep Learning","container-title":"IEEE Access","page":"82562-82570","volume":"8","abstract":"Traffic congestion is a thorny issue to many large and medium-sized cities, posing a serious threat to sustainable urban development. Recently, intelligent traffic system (ITS) has emerged as an effective tool to mitigate urban congestion. The key to the ITS lies in the accurate forecast of traffic flow. However, the existing forecast methods of traffic flow cannot adapt to the stochasticity and sheer length of traffic flow time series. To solve the problem, this paper relies on deep learning (DL) to forecast traffic flow through time series analysis. The authors developed a traffic flow forecast model based on the long short-term memory (LSTM) network. The proposed model was compared with two classic forecast models, namely, the autoregressive integrated moving average (ARIMA) model and the backpropagation neural network (BPNN) model, through long-term traffic flow forecast experiments, using an actual traffic flow time series from OpenITS. The experimental results show that the proposed LSTM network outperformed the classic models in prediction accuracy. Our research discloses the dynamic evolution law of traffic flow, and facilitates the decision-making of traffic management.","DOI":"10.1109/ACCESS.2020.2990738","note":"Conference Name: IEEE Access","author":[{"family":"Zheng","given":"J."},{"family":"Huang","given":"M."}],"issued":{"date-parts":[[2020]]}},"7765261/2QX25XCI":{"id":"7765261/2QX25XCI","type":"article","title":"Semantic Scholar Link","URL":"https://www.semanticscholar.org/paper/A-Review-of-Deep-Learning-Models-for-Time-Series-Han-Zhao/1bb052f153ff08f743574fa87c910d039f328ccc","accessed":{"date-parts":[[2021,4,14]]}},"7765261/VEH5EJ27":{"id":"7765261/VEH5EJ27","type":"article-journal","title":"A Review of Deep Learning Models for Time Series Prediction","container-title":"IEEE Sensors Journal","abstract":"In order to approximate the underlying process of temporal data, time series prediction has been a hot research topic for decades. Developing predictive models plays an important role in interpreting complex real-world elements. With the sharp increase in the quantity and dimensionality of data, new challenges, such as extracting deep features and recognizing deep latent patterns, have emerged, demanding novel approaches and effective solutions. Deep learning, composed of multiple processing layers to learn with multiple levels of abstraction, is, now, commonly deployed for overcoming the newly arisen difficulties. This paper reviews the state-of-the-art developments in deep learning for time series prediction. Based on modeling for the perspective of conditional or joint probability, we categorize them into discriminative, generative, and hybrids models. Experiments are implemented on both benchmarks and real-world data to elaborate the performance of the representative deep learning-based prediction methods. Finally, we conclude with comments on possible future perspectives and ongoing challenges with time series prediction.","DOI":"10.1109/JSEN.2019.2923982","author":[{"family":"Han","given":"Zhongyang"},{"family":"Zhao","given":"Jun"},{"family":"Leung","given":"H."},{"family":"Ma","given":"King Fai"},{"family":"Wang","given":"Wei"}],"issued":{"date-parts":[[2021]]}},"7765261/7A3TWY77":{"id":"7765261/7A3TWY77","type":"article","title":"Snapshot","URL":"https://www.semanticscholar.org/paper/An-Overview-of-Deep-Learning-Strategies-for-Time-Neves/1522b7439c935a6ecc4defbdb7d5e003268ecdd3","accessed":{"date-parts":[[2021,4,14]]}},"7765261/K3BPAGIU":{"id":"7765261/K3BPAGIU","type":"webpage","title":"An Overview of Deep Learning Strategies for Time Series Prediction","abstract":"Deep learning is getting a lot of attention in the last few years, mainly due to the state-of-the-art results obtained in different areas like object detection, natural language processing, sequential modeling, among many others. Time series problems are a special case of sequential data, where deep learning models can be applied. The standard option to this type of problems are Recurrent Neural Networks (RNNs), but recent results are supporting the idea that Convolutional Neural Networks (CNNs) can also be applied to time series with good results. This raises the following question Which are the best attributes and architectures to apply in time series prediction problems? It was assessed which is the current state on deep learning applied to time-series and studied which are the most promising topologies and characteristics on sequential tasks that are worth it to be explored. The study focused on two different time series problems, wind power forecasting and predictive maintenance. Both experiments were conducted under the same conditions across different models to guarantee a fair comparison basis. The study showed that different models and architectures can be applied on distinct time series problems with some level of success, thus showing the value and versatility of deep learning models in distinct areas. The results also showed that CNNs, together with recurrent architectures, are a viable option to apply in time series problems.","URL":"/paper/An-Overview-of-Deep-Learning-Strategies-for-Time-Neves/1522b7439c935a6ecc4defbdb7d5e003268ecdd3","language":"en","author":[{"family":"Neves","given":"Rodrigo"}],"issued":{"date-parts":[[2018]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/INH4FC9M":{"id":"7765261/INH4FC9M","type":"article","title":"Semantic Scholar Link","URL":"https://www.semanticscholar.org/paper/Modeling-Time-Series-Data-with-Deep-Learning%3A-A-and-Ang-Ng/c3d6ecc54cb317d8787943710007c43d1d4f663a","accessed":{"date-parts":[[2021,4,14]]}},"7765261/BXVLEGYT":{"id":"7765261/BXVLEGYT","type":"article-journal","title":"Modeling Time Series Data with Deep Learning: A Review, Analysis, Evaluation and Future Trend","container-title":"2020 8th International Conference on Information Technology and Multimedia (ICIMU)","abstract":"Time series modeling is a challenging and demanding problem. In the recent year, deep learning (DL) has attracted huge attention in many fields of research, including time series analysis and forecasting. While the methods of DL are very broad and wide, we aim to review the most recent and impactful deep learning papers in order to provide insights from the notable DL models and evaluation methods on time series problems. Our main objective is to review and analyse the advantages and disadvantages of different models, evaluation methods, future trends and techniques of solving time series problem with DL.","DOI":"10.1109/ICIMU49871.2020.9243546","shortTitle":"Modeling Time Series Data with Deep Learning","author":[{"family":"Ang","given":"John-Syin"},{"family":"Ng","given":"Kok-Why"},{"family":"Chua","given":"Fang-Fang"}],"issued":{"date-parts":[[2020]]}},"7765261/GUQ55L72":{"id":"7765261/GUQ55L72","type":"article","title":"Full Text","URL":"https://www.liebertpub.com/doi/pdf/10.1089/big.2020.0159","accessed":{"date-parts":[[2021,4,14]]}},"7765261/8MTTYUFN":{"id":"7765261/8MTTYUFN","type":"article","title":"PubMed entry","URL":"http://www.ncbi.nlm.nih.gov/pubmed/33275484","accessed":{"date-parts":[[2021,4,14]]}},"7765261/J8RSZ7UD":{"id":"7765261/J8RSZ7UD","type":"article-journal","title":"Deep Learning for Time Series Forecasting: A Survey","container-title":"Big Data","page":"3-21","volume":"9","issue":"1","abstract":"Time series forecasting has become a very intensive field of research, which is even increasing in recent years. Deep neural networks have proved to be powerful and are achieving high accuracy in many application fields. For these reasons, they are one of the most widely used methods of machine learning to solve problems dealing with big data nowadays. In this work, the time series forecasting problem is initially formulated along with its mathematical fundamentals. Then, the most common deep learning architectures that are currently being successfully applied to predict time series are described, highlighting their advantages and limitations. Particular attention is given to feed forward networks, recurrent neural networks (including Elman, long-short term memory, gated recurrent units, and bidirectional networks), and convolutional neural networks. Practical aspects, such as the setting of values for hyper-parameters and the choice of the most suitable frameworks, for the successful application of deep learning to time series are also provided and discussed. Several fruitful research fields in which the architectures analyzed have obtained a good performance are reviewed. As a result, research gaps have been identified in the literature for several domains of application, thus expecting to inspire new and better forms of knowledge.","DOI":"10.1089/big.2020.0159","note":"PMID: 33275484","shortTitle":"Deep Learning for Time Series Forecasting","journalAbbreviation":"Big Data","language":"eng","author":[{"family":"Torres","given":"José F."},{"family":"Hadjout","given":"Dalil"},{"family":"Sebaa","given":"Abderrazak"},{"family":"Martínez-Álvarez","given":"Francisco"},{"family":"Troncoso","given":"Alicia"}],"issued":{"date-parts":[[2021,2]]}},"7765261/6H8VZVUC":{"id":"7765261/6H8VZVUC","type":"article","title":"Submitted Version","URL":"https://arxiv.org/pdf/2103.12057","accessed":{"date-parts":[[2021,4,14]]}},"7765261/6ZVBKKLU":{"id":"7765261/6ZVBKKLU","type":"article","title":"PubMed entry","URL":"http://www.ncbi.nlm.nih.gov/pubmed/33588711","accessed":{"date-parts":[[2021,4,14]]}},"7765261/CMXCCQRV":{"id":"7765261/CMXCCQRV","type":"article-journal","title":"An Experimental Review on Deep Learning Architectures for Time Series Forecasting","container-title":"International Journal of Neural Systems","page":"2130001","volume":"31","issue":"3","abstract":"In recent years, deep learning techniques have outperformed traditional models in many machine learning tasks. Deep neural networks have successfully been applied to address time series forecasting problems, which is a very important topic in data mining. They have proved to be an effective solution given their capacity to automatically learn the temporal dependencies present in time series. However, selecting the most convenient type of deep neural network and its parametrization is a complex task that requires considerable expertise. Therefore, there is a need for deeper studies on the suitability of all existing architectures for different forecasting tasks. In this work, we face two main challenges: a comprehensive review of the latest works using deep learning for time series forecasting and an experimental study comparing the performance of the most popular architectures. The comparison involves a thorough analysis of seven types of deep learning models in terms of accuracy and efficiency. We evaluate the rankings and distribution of results obtained with the proposed models under many different architecture configurations and training hyperparameters. The datasets used comprise more than 50,000 time series divided into 12 different forecasting problems. By training more than 38,000 models on these data, we provide the most extensive deep learning study for time series forecasting. Among all studied models, the results show that long short-term memory (LSTM) and convolutional networks (CNN) are the best alternatives, with LSTMs obtaining the most accurate forecasts. CNNs achieve comparable performance with less variability of results under different parameter configurations, while also being more efficient.","DOI":"10.1142/S0129065721300011","note":"PMID: 33588711","journalAbbreviation":"Int J Neural Syst","language":"eng","author":[{"family":"Lara-Benítez","given":"Pedro"},{"family":"Carranza-García","given":"Manuel"},{"family":"Riquelme","given":"José C."}],"issued":{"date-parts":[[2021,3]]}},"7765261/H99AR8PE":{"id":"7765261/H99AR8PE","type":"article","title":"Full Text PDF","URL":"https://arxiv.org/pdf/1701.01887.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/LJCM75YX":{"id":"7765261/LJCM75YX","type":"article-journal","title":"Deep Learning for Time-Series Analysis","container-title":"ArXiv","abstract":"In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field.","author":[{"family":"Gamboa","given":"J."}],"issued":{"date-parts":[[2017]]}},"7765261/7BDL7PFT":{"id":"7765261/7BDL7PFT","type":"article","title":"Semantic Scholar Link","URL":"https://www.semanticscholar.org/paper/Deep-Learning-for-Time-Series-Analysis-Gamboa/5f3a79e184d4e4ebaee7fb5cd9a0304dd26a43b6","accessed":{"date-parts":[[2021,4,14]]}},"7765261/82NXSD2I":{"id":"7765261/82NXSD2I","type":"article","title":"IEEE Xplore Abstract Record","URL":"https://ieeexplore.ieee.org/document/9356200","accessed":{"date-parts":[[2021,4,14]]}},"7765261/D5DHLMM3":{"id":"7765261/D5DHLMM3","type":"paper-conference","title":"Deep Learning based Time Series Forecasting","container-title":"2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)","page":"859-864","event":"2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)","abstract":"For decision-makers in the forecasting sector, decision processes like planning of facilities, an optimal day-to-day operation within the domain etc., are complex with several different levels to be considered. These decisions address widely different time horizons and aspects of the system, making it difficult to model. The advent of deep learning in forecasting solved the need for expensive hand-crafted features and deep domain knowledge. The work aims at giving a structure to the existing literature for time-series forecasting in deep learning. Based on the underlying structures of the technique, such as RNN, CNN, and Transformer, we have categorized various deep learning-based time series forecasting techniques and provided a consolidated report. Additionally, we have performed experiments to compare these techniques on 4 different publicly available datasets. Finally, based on these experiments, we provide an intuitive reasoning behind these performances. We believe that this work shall help the researchers in choosing relevant techniques for future research.","DOI":"10.1109/ICMLA51294.2020.00140","author":[{"family":"Agarwal","given":"K."},{"family":"Dheekollu","given":"L."},{"family":"Dhama","given":"G."},{"family":"Arora","given":"A."},{"family":"Asthana","given":"S."},{"family":"Bhowmik","given":"T."}],"issued":{"date-parts":[["2020",12]]}},"7765261/IGIIJG36":{"id":"7765261/IGIIJG36","type":"article","title":"ScienceDirect Snapshot","URL":"https://www.sciencedirect.com/science/article/pii/S0169207006000021","accessed":{"date-parts":[[2021,4,14]]}},"7765261/GR2YSURK":{"id":"7765261/GR2YSURK","type":"article","title":"ScienceDirect Full Text PDF","URL":"https://pdf.sciencedirectassets.com/271676/1-s2.0-S0169207006X00833/1-s2.0-S0169207006000021/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCzAmuTOp4QxwqMT%2BmpBmJcwWD2toN7xIqNWRRcJGICzQIhAI98J3%2Fy9Uxqr%2Bz6khhuNAZMLAVsAXBtW04ahOiO0YuWKr0DCPD%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMDU5MDAzNTQ2ODY1IgwsmRhmjpss95qUIIYqkQMBqn4TvtFQG9Upx5zY21ht9cL71cY344RdE9kvU6p0T3cnEvh7qLgvkvnTwWjajexidVcF7UizV7053LoSHaU1sd%2FuQAOpQ4GEoqbnVF5Odo4FDPj9L2ZJCPJK4%2BnQh5DnJA6yocU6hnsszgJD1FSkduRgxyLB38td429CIRvVYPyL07WKeiBCenZpw9s4W05UHjb8M2ahaMucOProEs3ilGLOEuTzDxp1wPIfT50YOiXdLbqRmk%2BnzrCihCD4cKupqKGXHPLQwBR0F9VSqKDdJ60401tYaD%2FDYrbMY1rmiiiN8Xx%2BqSKvDQCejSFCgUS7KC4loFXRmhyemCsTuYRNUO0hJpXmrJaB%2Fy3%2F5PS2SCNuFopCJgSjjoUBj8VP0CwRQ%2FRDexzni%2F7X0GObwmLJFf5TMl5lFN6%2Bhmc2c2uaRrgg8VLA1HbfYfnTqTaBj2qAR65Ge6VTn845ApUARt8SmDeLaZ%2FdP1pTV%2FqQBoDlztU4rs1LVX4hektP9LYueWtoNjhKWOSsH7sDJaVVcQzQJjCR%2FtuDBjrqATVR7gdlRFll6Wf4vCGgWCji3gVl6m8rsx7r%2BVbyqftM%2BEY9mQPdwK37lMp1LVynISw5g6neU4JRMKpC5WGcYJrtwCUHfQCgs1IdFlb2V0iwKFSFx8a6OCiHqehI%2BbAygdnQecM5e%2BysN0wVnjMabJSvsyGkRoAIljs7WcZFp%2FECiM9ADt9%2FeIEXTAU9vot2YmB2jtnObmHXnbE2YJfHdewO04w6KOIdMbgC9Imsg5GfhQpk0pmLepL7G0deEsD0fGq%2Bmu5%2F3FDd0kuf0CHr7hll26tmG6PdFVnsnG4soPJAMPaey3VpyfxsXg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210414T153822Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYY5BL2NJ7%2F20210414%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=bc86c8a927f1e0dde174c0cd35691ba500023a8efea06dfc65daffdd2b1ef597&hash=f3a085b4826bcced4611e34b08a86bd41b548931b438620fd3417ddc1f737a83&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0169207006000021&tid=spdf-d1969fab-72f0-4664-a421-38b7306db217&sid=85d04e1c4e52c544a37875e0afc3a974e051gxrqb&type=client","accessed":{"date-parts":[[2021,4,14]]}},"7765261/6RY59JBP":{"id":"7765261/6RY59JBP","type":"article-journal","title":"25 years of time series forecasting","container-title":"International Journal of Forecasting","collection-title":"Twenty five years of forecasting","page":"443-473","volume":"22","issue":"3","abstract":"We review the past 25 years of research into time series forecasting. In this silver jubilee issue, we naturally highlight results published in journals managed by the International Institute of Forecasters (Journal of Forecasting 1982–1985 and International Journal of Forecasting 1985–2005). During this period, over one third of all papers published in these journals concerned time series forecasting. We also review highly influential works on time series forecasting that have been published elsewhere during this period. Enormous progress has been made in many areas, but we find that there are a large number of topics in need of further development. We conclude with comments on possible future research directions in this field.","URL":"https://www.sciencedirect.com/science/article/pii/S0169207006000021","DOI":"10.1016/j.ijforecast.2006.01.001","journalAbbreviation":"International Journal of Forecasting","language":"en","author":[{"family":"De Gooijer","given":"Jan G."},{"family":"Hyndman","given":"Rob J."}],"issued":{"date-parts":[["2006",1,1]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/LEUK4NCX":{"id":"7765261/LEUK4NCX","type":"article","title":"ScienceDirect Full Text PDF","URL":"https://pdf.sciencedirectassets.com/271676/1-s2.0-S0169207019X00047/1-s2.0-S0169207019301153/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCzAmuTOp4QxwqMT%2BmpBmJcwWD2toN7xIqNWRRcJGICzQIhAI98J3%2Fy9Uxqr%2Bz6khhuNAZMLAVsAXBtW04ahOiO0YuWKr0DCPD%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMDU5MDAzNTQ2ODY1IgwsmRhmjpss95qUIIYqkQMBqn4TvtFQG9Upx5zY21ht9cL71cY344RdE9kvU6p0T3cnEvh7qLgvkvnTwWjajexidVcF7UizV7053LoSHaU1sd%2FuQAOpQ4GEoqbnVF5Odo4FDPj9L2ZJCPJK4%2BnQh5DnJA6yocU6hnsszgJD1FSkduRgxyLB38td429CIRvVYPyL07WKeiBCenZpw9s4W05UHjb8M2ahaMucOProEs3ilGLOEuTzDxp1wPIfT50YOiXdLbqRmk%2BnzrCihCD4cKupqKGXHPLQwBR0F9VSqKDdJ60401tYaD%2FDYrbMY1rmiiiN8Xx%2BqSKvDQCejSFCgUS7KC4loFXRmhyemCsTuYRNUO0hJpXmrJaB%2Fy3%2F5PS2SCNuFopCJgSjjoUBj8VP0CwRQ%2FRDexzni%2F7X0GObwmLJFf5TMl5lFN6%2Bhmc2c2uaRrgg8VLA1HbfYfnTqTaBj2qAR65Ge6VTn845ApUARt8SmDeLaZ%2FdP1pTV%2FqQBoDlztU4rs1LVX4hektP9LYueWtoNjhKWOSsH7sDJaVVcQzQJjCR%2FtuDBjrqATVR7gdlRFll6Wf4vCGgWCji3gVl6m8rsx7r%2BVbyqftM%2BEY9mQPdwK37lMp1LVynISw5g6neU4JRMKpC5WGcYJrtwCUHfQCgs1IdFlb2V0iwKFSFx8a6OCiHqehI%2BbAygdnQecM5e%2BysN0wVnjMabJSvsyGkRoAIljs7WcZFp%2FECiM9ADt9%2FeIEXTAU9vot2YmB2jtnObmHXnbE2YJfHdewO04w6KOIdMbgC9Imsg5GfhQpk0pmLepL7G0deEsD0fGq%2Bmu5%2F3FDd0kuf0CHr7hll26tmG6PdFVnsnG4soPJAMPaey3VpyfxsXg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210414T153743Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYY5BL2NJ7%2F20210414%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8e231ed08e5bc09ceb3f7ea49fe33b5dbf19c4c22316e57790bb6aff045141c1&hash=c8ecd61b4f5e4818a680b7560cc1169c9093ed89b8f1cb022e5e6c7578fd0944&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0169207019301153&tid=spdf-38b164e9-4d95-4d7e-b100-64d8786409ab&sid=85d04e1c4e52c544a37875e0afc3a974e051gxrqb&type=client","accessed":{"date-parts":[[2021,4,14]]}},"7765261/AFV77T2C":{"id":"7765261/AFV77T2C","type":"article-journal","title":"A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting","container-title":"International Journal of Forecasting","collection-title":"M4 Competition","page":"75-85","volume":"36","issue":"1","abstract":"This paper presents the winning submission of the M4 forecasting competition. The submission utilizes a dynamic computational graph neural network system that enables a standard exponential smoothing model to be mixed with advanced long short term memory networks into a common framework. The result is a hybrid and hierarchical forecasting method.","URL":"https://www.sciencedirect.com/science/article/pii/S0169207019301153","DOI":"10.1016/j.ijforecast.2019.03.017","journalAbbreviation":"International Journal of Forecasting","language":"en","author":[{"family":"Smyl","given":"Slawek"}],"issued":{"date-parts":[["2020",1,1]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/WMF7XP52":{"id":"7765261/WMF7XP52","type":"article","title":"ScienceDirect Snapshot","URL":"https://www.sciencedirect.com/science/article/pii/S0169207019301876","accessed":{"date-parts":[[2021,4,14]]}},"7765261/PRCHP8XN":{"id":"7765261/PRCHP8XN","type":"article","title":"ScienceDirect Full Text PDF","URL":"https://pdf.sciencedirectassets.com/271676/1-s2.0-S0169207019X00047/1-s2.0-S0169207019301876/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIBIZYuQQcHsToCnhjfXt%2B%2FM%2FnTZ8s7cL%2FDQwkKn%2Fy4PYAiAPqtc3VT4j%2B3pWuFibE6CGCD7E4FCmXN5lgHi%2BjAeATiq9Awjv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAMaDDA1OTAwMzU0Njg2NSIMfT7ZuuCZJRDt2GYfKpEDxhh7bVUTQGV%2Ba4WSRvIB5f2%2Fn1FkfWqZgRs0KUa91bbgjWfsIAbgKv%2BPnuwzkx2Us2wsmlD4ajeiLhv8kSWTBKBTINgd4D37h4rHZqUSoUNRosoi0xz6wXxvfNbaDokd0BD2bCN44cqvHJ3gHIP%2BrAZwLF6CNQd8Fd3oqK2%2F0tJFLDN59ot2CsXKsEj84FgkVKdr7LWwtmPnLsHlv6HY27WKfaRNCgDZnJV5wUF9AzeB2RZzgE7pNOgHq48ZeEz3inY1J%2BbbBVmLfNohBJmqlLVV1OpcSSse8HLOWJVN8TQZcNCUZnKbJpy%2B%2BqPEsQ3aHvAkrwW1bmLfTTaMZ9tB0XYsL8J7aBEJU%2Bnr2lFkGpR6UWR3%2BoxhErP0%2FOC0GOGHVrQLILn5FR4xiqEkoovUQyulfR%2BX6IuIrtclTfyz6XZ5tmPQit%2BMBych4Hi9rJK6uHq8JVcZ%2BFKgSTVg1j44vIL4gYjTqMjYfkeVoiRgwMvndcrQN2yfSbS%2BYBZzU7xdUUWVpJsO%2B0X7tUz8tYgvVK8w6e%2FbgwY67AHkIVcrWnigQKonsO1bCTfvn44X4ribYWbLtMQygwAsI1Fzuyl4w5UQEX12hrhJIIlfHiCDJjF62Lmks5Qn%2F3GjBiKM3C2W3zP90T6%2FQZ6CD5suhyTI2e064RLpkLUeTZsuw9zim1P5i8otT%2FSMWHu%2BKR%2BV%2B%2B4X4PveYRIKxahT5uPQBwcsMcY3JKgwxp034K41ZUsqQCI6kdAVEi876RTKN%2Fw2SsRtFq26TpfEq7OyQYNSZ%2ByPFdYX8ijuQee7fbOAqqum6SVoZQu17CH%2Ffb2bJeNPAoewFJ9mlcERp0Dpupj7%2B1pTN3Inrc%2FVBg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210414T153537Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JCRCXU6%2F20210414%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=4562136bec97bc5df150538db9da07f971c99e4c67eb511acc9dacd07e8f27ef&hash=4f9fa78f83c2e0d776ccb759010614687cf9520ccecf02b18b3d18021f2a06e8&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0169207019301876&tid=spdf-d6145009-1557-4554-b691-f807b95c120b&sid=85d04e1c4e52c544a37875e0afc3a974e051gxrqb&type=client","accessed":{"date-parts":[[2021,4,14]]}},"7765261/SYDJRYXT":{"id":"7765261/SYDJRYXT","type":"article-journal","title":"Forecasting in social settings: The state of the art","container-title":"International Journal of Forecasting","collection-title":"M4 Competition","page":"15-28","volume":"36","issue":"1","abstract":"This paper provides a non-systematic review of the progress of forecasting in social settings. It is aimed at someone outside the field of forecasting who wants to understand and appreciate the results of the M4 Competition, and forms a survey paper regarding the state of the art of this discipline. It discusses the recorded improvements in forecast accuracy over time, the need to capture forecast uncertainty, and things that can go wrong with predictions. Subsequently, the review classifies the knowledge achieved over recent years into (i) what we know, (ii) what we are not sure about, and (iii) what we don’t knowIn the first two areas, we explore the difference between explanation and prediction, the existence of an optimal model, the performance of machine learning methods on time series forecasting tasks, the difficulties of predicting non-stable environments, the performance of judgment, and the value added by exogenous variables. The article concludes with the importance of (thin and) fat tails, the challenges and advances in causal inference, and the role of luck.","URL":"https://www.sciencedirect.com/science/article/pii/S0169207019301876","DOI":"10.1016/j.ijforecast.2019.05.011","shortTitle":"Forecasting in social settings","journalAbbreviation":"International Journal of Forecasting","language":"en","author":[{"family":"Makridakis","given":"Spyros"},{"family":"Hyndman","given":"Rob J."},{"family":"Petropoulos","given":"Fotios"}],"issued":{"date-parts":[["2020",1,1]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/DUU4E89T":{"id":"7765261/DUU4E89T","type":"article","title":"Talagala et al. - 2018 - Meta-learning how to forecast time series.pdf","URL":"https://www.monash.edu/business/ebs/research/publications/ebs/wp06-2018.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/TLYDGHE4":{"id":"7765261/TLYDGHE4","type":"article-journal","title":"Meta-learning how to forecast time series","page":"30","abstract":"A crucial task in time series forecasting is the identiﬁcation of the most suitable forecasting method. We present a general framework for forecast-model selection using meta-learning. A random forest is used to identify the best forecasting method using only time series features. The framework is evaluated using time series from the M1 and M3 competitions and is shown to yield accurate forecasts comparable to several benchmarks and other commonly used automated approaches of time series forecasting. A key advantage of our proposed framework is that the time-consuming process of building a classiﬁer is handled in advance of the forecasting task at hand.","language":"en","author":[{"family":"Talagala","given":"Thiyanga S"},{"family":"Hyndman","given":"Rob J"},{"family":"Athanasopoulos","given":"George"}],"issued":{"date-parts":[[2018]]}},"7765261/Q75RZH6J":{"id":"7765261/Q75RZH6J","type":"article","title":"ScienceDirect Snapshot","URL":"https://www.sciencedirect.com/science/article/pii/S016920701930086X#!","accessed":{"date-parts":[[2021,4,14]]}},"7765261/BTDLE9ZV":{"id":"7765261/BTDLE9ZV","type":"article","title":"ScienceDirect Full Text PDF","URL":"https://pdf.sciencedirectassets.com/271676/1-s2.0-S0169207019X00047/1-s2.0-S016920701930086X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCICZ0gdzD%2BXUGGwQTuc1da9nUl1rSg1T8wYiSsLMxYSCIAiEAhJolYmVEoWhi2QBLERxhXqszCxWmVLmeIbHnAQm8ByEqvQMI8P%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwwNTkwMDM1NDY4NjUiDLKR1CpD5qlYxL7ioyqRA1sALCyslRkKYQzBRijohr914PWv9YLDBeTj%2BQzVidnJLMcSziECG1Ec6xAeeeKDWbOciNFFaVsXWL%2Fb7jPEQNLiDzB5nv2kSQSFQV46QEZcUaf%2Fr%2FBCGnEFuxDS2zsmIzCdG%2BehplfcRvRKXgyo6ES53J2y%2FHLJC9JtDzMIvdthhPcUiX1M0fN4o%2Ff2Xigu7KYbtvG1ovsZjqjKENTi4t%2BAmNNdVn8ljXNJaCe%2B%2BNcpJcXyZVC1YiThEPQqTED1ZSdDsdzG%2Fme%2BVV8ri0gMVyK8G2VJ6wewKDtXPuBqDBAJUYU4QlibpyKDQksjILP1kaE6wkwafypG67Vy%2FhHhS7LHz9q%2FHeWA9rNvBQ5Gxukjbp%2BS1pezkyjW%2B%2Bl%2BgW3EmywmSH060%2BRZKHj52kdYfm6W6u1pYwsM55gZQPhVjGn4o6k4%2BRy%2FgD8FPleEylLVA8jSqwHQFMRKRzs1SdR%2FWAy3QDOC8lrT6HOGYSOpS3op3F2HVjU7JyPlUljpoHw6u%2F23sDdMOkKUZDmKYTpikmCMMMj924MGOusB4JufPdxjd1akWuK%2BhgHO0h%2FMABUOaUBgK%2BH20oNXFGb3aHLxhhX5oNTp81MRBTD1NLo4gCXzFcQC7NkynbvJsYJYYfrxKKFi7RRLy8%2FxgeT3Ciw3BzAwO8sBU9xRgydUST2%2BBsd%2BmOuzKHoAAP1Tj%2B9q743QaPRfbLtUt85ExhAeYl5Gvf2uFvXI7fuNszB44aSF%2BNEJYcYR6Ms0q2FmiBDMAqdbdUodJeA5nk8nhfrqqlRa7rKF%2F%2ByDYWym8E9YvaDZ2Z3I5bBcVZmEol47NgtiYfSIPw1K%2B6N8ahpxnmdf%2FziRt3oCSoh%2BDQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210414T152944Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY4SRL5HND%2F20210414%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0033ca6554d2f0c99fc41bff1d1716505f86ece93782a3da6232392ad4244d09&hash=eefee0703f8f3a09a7c4abe0a53ab284c2aa11698981c7efe4ecc06ef681b3f0&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S016920701930086X&tid=spdf-ef171530-d785-48bf-b362-431b7c1737ad&sid=85d04e1c4e52c544a37875e0afc3a974e051gxrqb&type=client","accessed":{"date-parts":[[2021,4,14]]}},"7765261/LJCNMWP5":{"id":"7765261/LJCNMWP5","type":"article-journal","title":"A brief history of forecasting competitions","container-title":"International Journal of Forecasting","collection-title":"M4 Competition","page":"7-14","volume":"36","issue":"1","abstract":"Forecasting competitions are now so widespread that it is often forgotten how controversial they were when first held, and how influential they have been over the years. I briefly review the history of forecasting competitions, and discuss what we have learned about their design and implementation, and what they can tell us about forecasting. I also provide a few suggestions for potential future competitions, and for research about forecasting based on competitions.","URL":"https://www.sciencedirect.com/science/article/pii/S016920701930086X","DOI":"10.1016/j.ijforecast.2019.03.015","journalAbbreviation":"International Journal of Forecasting","language":"en","author":[{"family":"Hyndman","given":"Rob J."}],"issued":{"date-parts":[["2020",1,1]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/96P8LMSH":{"id":"7765261/96P8LMSH","type":"article","title":"ScienceDirect Full Text PDF","URL":"https://pdf.sciencedirectassets.com/271676/1-s2.0-S0169207020X00057/1-s2.0-S0169207020300996/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCICZ0gdzD%2BXUGGwQTuc1da9nUl1rSg1T8wYiSsLMxYSCIAiEAhJolYmVEoWhi2QBLERxhXqszCxWmVLmeIbHnAQm8ByEqvQMI8P%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwwNTkwMDM1NDY4NjUiDLKR1CpD5qlYxL7ioyqRA1sALCyslRkKYQzBRijohr914PWv9YLDBeTj%2BQzVidnJLMcSziECG1Ec6xAeeeKDWbOciNFFaVsXWL%2Fb7jPEQNLiDzB5nv2kSQSFQV46QEZcUaf%2Fr%2FBCGnEFuxDS2zsmIzCdG%2BehplfcRvRKXgyo6ES53J2y%2FHLJC9JtDzMIvdthhPcUiX1M0fN4o%2Ff2Xigu7KYbtvG1ovsZjqjKENTi4t%2BAmNNdVn8ljXNJaCe%2B%2BNcpJcXyZVC1YiThEPQqTED1ZSdDsdzG%2Fme%2BVV8ri0gMVyK8G2VJ6wewKDtXPuBqDBAJUYU4QlibpyKDQksjILP1kaE6wkwafypG67Vy%2FhHhS7LHz9q%2FHeWA9rNvBQ5Gxukjbp%2BS1pezkyjW%2B%2Bl%2BgW3EmywmSH060%2BRZKHj52kdYfm6W6u1pYwsM55gZQPhVjGn4o6k4%2BRy%2FgD8FPleEylLVA8jSqwHQFMRKRzs1SdR%2FWAy3QDOC8lrT6HOGYSOpS3op3F2HVjU7JyPlUljpoHw6u%2F23sDdMOkKUZDmKYTpikmCMMMj924MGOusB4JufPdxjd1akWuK%2BhgHO0h%2FMABUOaUBgK%2BH20oNXFGb3aHLxhhX5oNTp81MRBTD1NLo4gCXzFcQC7NkynbvJsYJYYfrxKKFi7RRLy8%2FxgeT3Ciw3BzAwO8sBU9xRgydUST2%2BBsd%2BmOuzKHoAAP1Tj%2B9q743QaPRfbLtUt85ExhAeYl5Gvf2uFvXI7fuNszB44aSF%2BNEJYcYR6Ms0q2FmiBDMAqdbdUodJeA5nk8nhfrqqlRa7rKF%2F%2ByDYWym8E9YvaDZ2Z3I5bBcVZmEol47NgtiYfSIPw1K%2B6N8ahpxnmdf%2FziRt3oCSoh%2BDQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210414T152926Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY4SRL5HND%2F20210414%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=4ddd7de5d1a7021e3184bb3447e3c32d2ededa47395c9f6767978e0ef5ea7e19&hash=3fde91688717d976bf8a5583103b51fbee84048e93746e11b9795a70076e4ae6&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0169207020300996&tid=spdf-5e44a94d-530d-4b04-acd2-c61a02848411&sid=85d04e1c4e52c544a37875e0afc3a974e051gxrqb&type=client","accessed":{"date-parts":[[2021,4,14]]}},"7765261/GK995VCQ":{"id":"7765261/GK995VCQ","type":"article-journal","title":"Recurrent Neural Networks for Time Series Forecasting: Current status and future directions","container-title":"International Journal of Forecasting","page":"388-427","volume":"37","issue":"1","abstract":"Recurrent Neural Networks (RNNs) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as exponential smoothing (ETS) and the autoregressive integrated moving average (ARIMA) gain their popularity not only from their high accuracy, but also because they are suitable for non-expert users in that they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, and we develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns; otherwise, we recommend a deseasonalisation step. Comparisons against ETS and ARIMA demonstrate that (semi-) automatic RNN models are not silver bullets, but they are nevertheless competitive alternatives in many situations.","URL":"https://www.sciencedirect.com/science/article/pii/S0169207020300996","DOI":"10.1016/j.ijforecast.2020.06.008","shortTitle":"Recurrent Neural Networks for Time Series Forecasting","journalAbbreviation":"International Journal of Forecasting","language":"en","author":[{"family":"Hewamalage","given":"Hansika"},{"family":"Bergmeir","given":"Christoph"},{"family":"Bandara","given":"Kasun"}],"issued":{"date-parts":[["2021",1,1]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/LWZ97WW6":{"id":"7765261/LWZ97WW6","type":"article","title":"Springer Full Text PDF","URL":"https://link.springer.com/content/pdf/10.1007%2F978-3-030-31150-6_6.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/HNSR6DEW":{"id":"7765261/HNSR6DEW","type":"chapter","title":"Neural Networks","container-title":"Macroeconomic Forecasting in the Era of Big Data: Theory and Practice","collection-title":"Advanced Studies in Theoretical and Applied Econometrics","publisher":"Springer International Publishing","publisher-place":"Cham","page":"161-189","event-place":"Cham","abstract":"In the past 10 years, neural networks have emerged as a powerful tool for predictive modeling with “big data.” This chapter discusses the potential role of neural networks as applied to economic forecasting. It begins with a brief discussion of the history of neural networks, their use in economics, and their value as universal function approximators. It proceeds to introduce the elemental structures of neural networks, taking the classic feed forward, fully connected type of neural network as its point of reference. A broad set of design decisions are discussed including regularization, activation functions, and model architecture. Following this, two additional types of neural network model are discussed: recurrent neural networks and encoder-decoder models. The chapter concludes with an empirical application of all three models to the task of forecasting unemployment.","URL":"https://doi.org/10.1007/978-3-030-31150-6_6","ISBN":"978-3-030-31150-6","note":"DOI: 10.1007/978-3-030-31150-6_6","language":"en","author":[{"family":"Cook","given":"Thomas R."}],"editor":[{"family":"Fuleky","given":"Peter"}],"issued":{"date-parts":[[2020]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/JBHERP4I":{"id":"7765261/JBHERP4I","type":"article","title":"Submitted Version","URL":"https://www.monash.edu/business/ebs/research/publications/ebs/wp02-2019.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/TJP8KWNT":{"id":"7765261/TJP8KWNT","type":"chapter","title":"Hierarchical Forecasting","container-title":"Macroeconomic Forecasting in the Era of Big Data: Theory and Practice","collection-title":"Advanced Studies in Theoretical and Applied Econometrics","publisher":"Springer International Publishing","publisher-place":"Cham","page":"689-719","event-place":"Cham","abstract":"Accurate forecasts of macroeconomic variables are crucial inputs into the decisions of economic agents and policy makers. Exploiting inherent aggregation structures of such variables, we apply forecast reconciliation methods to generate forecasts that are coherent with the aggregation constraints. We generate both point and probabilistic forecasts for the first time in the macroeconomic setting. Using Australian GDP we show that forecast reconciliation not only returns coherent forecasts but also improves the overall forecast accuracy in both point and probabilistic frameworks.","URL":"https://doi.org/10.1007/978-3-030-31150-6_21","ISBN":"978-3-030-31150-6","note":"DOI: 10.1007/978-3-030-31150-6_21","language":"en","author":[{"family":"Athanasopoulos","given":"George"},{"family":"Gamakumara","given":"Puwasala"},{"family":"Panagiotelis","given":"Anastasios"},{"family":"Hyndman","given":"Rob J."},{"family":"Affan","given":"Mohamed"}],"editor":[{"family":"Fuleky","given":"Peter"}],"issued":{"date-parts":[[2020]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/TCMU9CWC":{"id":"7765261/TCMU9CWC","type":"article","title":"Submitted Version","URL":"https://uhero.hawaii.edu/wp-content/uploads/2019/07/UHEROwp1903.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/NPURTXQ8":{"id":"7765261/NPURTXQ8","type":"chapter","title":"Sources and Types of Big Data for Macroeconomic Forecasting","container-title":"Macroeconomic Forecasting in the Era of Big Data: Theory and Practice","collection-title":"Advanced Studies in Theoretical and Applied Econometrics","publisher":"Springer International Publishing","publisher-place":"Cham","page":"3-23","event-place":"Cham","abstract":"This chapter considers the types of Big Data that have proven useful for macroeconomic forecasting. It first presents the various definitions of Big Data, proposing one we believe is most useful for forecasting. The literature on both the opportunities and challenges of Big Data are presented. It then proposes a taxonomy of the types of Big Data: (1) Financial Market Data; (2) E-Commerce and Credit Cards; (3) Mobile Phones; (4) Search; (5) Social Media Data; (6) Textual Data; (7) Sensors, and The Internet of Things; (8) Transportation Data; (9) Other Administrative Data. Noteworthy studies are described throughout.","URL":"https://doi.org/10.1007/978-3-030-31150-6_1","ISBN":"978-3-030-31150-6","note":"DOI: 10.1007/978-3-030-31150-6_1","language":"en","author":[{"family":"Garboden","given":"Philip M. E."}],"editor":[{"family":"Fuleky","given":"Peter"}],"issued":{"date-parts":[[2020]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/WFKD4TTI":{"id":"7765261/WFKD4TTI","type":"article","title":"arXiv.org Snapshot","URL":"https://arxiv.org/abs/1912.00370","accessed":{"date-parts":[[2021,4,14]]}},"7765261/U47ULA9H":{"id":"7765261/U47ULA9H","type":"article","title":"arXiv Fulltext PDF","URL":"https://arxiv.org/pdf/1912.00370.pdf","accessed":{"date-parts":[[2021,4,14]]}},"7765261/2NINX6DD":{"id":"7765261/2NINX6DD","type":"article-journal","title":"Machine learning applications in time series hierarchical forecasting","container-title":"arXiv:1912.00370 [cs, stat]","abstract":"Hierarchical forecasting (HF) is needed in many situations in the supply chain (SC) because managers often need different levels of forecasts at different levels of SC to make a decision. Top-Down (TD), Bottom-Up (BU) and Optimal Combination (COM) are common HF models. These approaches are static and often ignore the dynamics of the series while disaggregating them. Consequently, they may fail to perform well if the investigated group of time series are subject to large changes such as during the periods of promotional sales. We address the HF problem of predicting real-world sales time series that are highly impacted by promotion. We use three machine learning (ML) models to capture sales variations over time. Artificial neural networks (ANN), extreme gradient boosting (XGboost), and support vector regression (SVR) algorithms are used to estimate the proportions of lower-level time series from the upper level. We perform an in-depth analysis of 61 groups of time series with different volatilities and show that ML models are competitive and outperform some well-established models in the literature.","URL":"http://arxiv.org/abs/1912.00370","note":"arXiv: 1912.00370","author":[{"family":"Abolghasemi","given":"Mahdi"},{"family":"Hyndman","given":"Rob J."},{"family":"Tarr","given":"Garth"},{"family":"Bergmeir","given":"Christoph"}],"issued":{"date-parts":[[2019,12,1]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/G4XZBURR":{"id":"7765261/G4XZBURR","type":"chapter","title":"Neural Networks","container-title":"Macroeconomic Forecasting in the Era of Big Data: Theory and Practice","collection-title":"Advanced Studies in Theoretical and Applied Econometrics","publisher":"Springer International Publishing","publisher-place":"Cham","page":"161-189","event-place":"Cham","abstract":"In the past 10 years, neural networks have emerged as a powerful tool for predictive modeling with “big data.” This chapter discusses the potential role of neural networks as applied to economic forecasting. It begins with a brief discussion of the history of neural networks, their use in economics, and their value as universal function approximators. It proceeds to introduce the elemental structures of neural networks, taking the classic feed forward, fully connected type of neural network as its point of reference. A broad set of design decisions are discussed including regularization, activation functions, and model architecture. Following this, two additional types of neural network model are discussed: recurrent neural networks and encoder-decoder models. The chapter concludes with an empirical application of all three models to the task of forecasting unemployment.","URL":"https://doi.org/10.1007/978-3-030-31150-6_6","ISBN":"978-3-030-31150-6","note":"DOI: 10.1007/978-3-030-31150-6_6","language":"en","author":[{"family":"Cook","given":"Thomas R."}],"editor":[{"family":"Fuleky","given":"Peter"}],"issued":{"date-parts":[[2020]]},"accessed":{"date-parts":[[2021,4,14]]}},"7765261/TJQFRM5R":{"id":"7765261/TJQFRM5R","type":"article","title":"Snapshot","URL":"https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0209","accessed":{"date-parts":[[2021,4,13]]}},"7765261/3LKCB9CB":{"id":"7765261/3LKCB9CB","type":"article","title":"Full Text PDF","URL":"https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2020.0209","accessed":{"date-parts":[[2021,4,13]]}}}}},"metadata":{"id":"tesi","last_modified":"2021-08-30T07:19:21.438254+00:00","created":"2021-08-30T07:19:21.438254+00:00"}}